#  Comlex Osint model a terminer a la fin . +
import argparse

from Projet_Fin_Etude.Utilities.Core.colors import *
from Projet_Fin_Etude.Utilities.Core.regex import *
from Projet_Fin_Etude.Utilities.Core.requester import *
from pyfiglet import Figlet
import sys
import re

'''
-Added handle argument of users  :
 -Setting default values of parameters: OK.
 -URL : Working ...
 -Extract internal urls : Working ...
 -Proxies : Not important (Option)
'''

keys = set()
files = set()
intel = set()
robots = set()
custom = set()
scripts = set()
external = set()
internal = set()
endpoints = set()
processed = set(['dummy'])
everything = []
bad_script = set()
bad_intel = set()

# Just a fancy ass banner
'''
Should be added in a function that switch modes /
'''
custom_banner = Figlet(font='contessa')
print(custom_banner.renderText('OSINT_S'))

'''
This function should return all the argument that was provided by the user :
//At this moment inside the function a tested the fact that we can see what the user provide us :URL
//pased_args contains all the args
'''


def handle_user_parameters(args=None):
    if args is None:
        args = sys.argv[1:]
    parser = argparse.ArgumentParser()
    parser.add_argument("-u", "--url", help="root url")
    # Contains all arguments
    parsed_args = parser.parse_args(args)
    # We can remove this
    url = parsed_args.url
    if str(url).endswith('/'):
        url = url[:-1]
    else:
        quit()
    print(url)
    return parsed_args


def is_good_proxy(proxy):
    return False

    '''
    A revoir
    // Not that correct and not important
    '''


def handle_proxies(proxies):
    if handle_user_parameters().proxies:
        print("Testing proxies, can take a while ....")
        for proxy in handle_user_parameters().proxies:
            if is_good_proxy(proxy):
                proxies.append(proxy)
            else:
                print("Proxy doesn't seem to work or timout")
        print("Done.")
        if not proxies:
            print("no working proxies, quitting!")
            exit()
    else:
        proxies.append(None)

    # Define some default values for parameters:
    crawl_level = handle_user_parameters().level or 2
    number_thread = handle_user_parameters().threads or 2
    only_urls_mod = bool(handle_user_parameters().only_urls)


def extract_internal_urls(url, response):
    # Strip out any script tags and their contents
    response = re.sub(r'<(script).*?</\1>(?s)', '', response)
    # Strip out any remaining HTML tags
    response = re.sub(r'<[^<]+?>', '', response)
    for rintel in rintels:
        matches = rintel[0].findall(response)
        if matches:
            for match in matches:
                # verb('Intel', match) this should be imported from uttilities
                print("Internal links ", match)
                bad_intel.add((match, rintel[1], url))


def js_extractor(response):
    matches = rscript.findall(response)
    for match in matches:
        match = match[2].replace('\',''').replace('"', '')
        # verb('Js file',match) //Should be added later this format the output.
        print('Js file :', match)
        bad_script.add(match)

def extract_internal_external(url):
    response = requester(url)
    if handle_user_parameters():
        mirror(url, response)
    matches = rhref.findall(response)
    for link in matches:
        # Remove everything after a "#" to deal with in-page anchors
        link = link[1].replace('\'', '').replace('"', '').split('#')[0]
        # Checks if the URLs should be crawled
        if is_link(link, processed, files):
            if link[:4] == 'http':
                if link.startswith(main_url):
                    #verb('Internal page', link)
                    internal.add(link)
                else:
                    #verb('External page', link)
                    external.add(link)
            elif link[:2] == '//':
                if link.split('/')[2].startswith(host):
                    #verb('Internal page', link)
                    internal.add(schema + '://' + link)
                else:
                    #verb('External page', link)
                    external.add(link)
            elif link[:1] == '/':
                #verb('Internal page', link)
                internal.add(remove_file(url) + link)
            else:
                #verb('Internal page', link)
                usable_url = remove_file(url)
                if usable_url.endswith('/'):
                    internal.add(usable_url + link)
                elif link.startswith('/'):
                    internal.add(usable_url + link)
                else:
                    internal.add(usable_url + '/' + link)

    if not only_urls:
        intel_extractor(url, response)
        js_extractor(response)
    if args.regex and not supress_regex:
        regxy(args.regex, response, supress_regex, custom)
    if api:
        matches = rentropy.findall(response)
        for match in matches:
            if entropy(match) >= 4:
                verb('Key', match)
                keys.add(url + ': ' + match)